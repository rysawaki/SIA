# Self-Conditioned Attention  
### â€• è‡ªå·±çŠ¶æ…‹ã‚’Attentionã«ç›´æ¥ä»‹å…¥ã•ã›ã‚‹æœ€å°æ§‹é€  â€•

**è‘—è€…:** *ï¼ˆã‚ãªãŸã®åå‰ã‚’å…¥ã‚Œã¦ãã ã•ã„ï¼‰*  
**æ—¥ä»˜:** 2025-11-28  

---

## æ¦‚è¦ï¼ˆAbstractï¼‰

æœ¬ç¨¿ã§ã¯ã€Transformer ã® Attention ã«ã€ŒSelfï¼ˆè‡ªå·±çŠ¶æ…‹ï¼‰ã€ã‚’æœ€å°ã‚³ã‚¹ãƒˆã§çµ„ã¿è¾¼ã‚€ **Self-Conditioned Attention** ã‚’ææ¡ˆã™ã‚‹ã€‚  
é€šå¸¸ã® Attention ã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒå£«ã®ç›¸å¯¾é–¢ä¿‚ã®ã¿ã‚’æ‰±ã„ã€ãƒ¢ãƒ‡ãƒ«è‡ªèº«ã®å±¥æ­´ãƒ»ç—•è·¡ãƒ»å†…éƒ¨çŠ¶æ…‹ã‚’ç›´æ¥çš„ã«åæ˜ ã—ãªã„ã€‚  
æœ¬æ‰‹æ³•ã§ã¯ã€ä½æ¬¡å…ƒã®è‡ªå·±çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« \( s_t \) ã‚’ Attention ã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—ã«ç›´æ¥ä»‹å…¥ã•ã›ã‚‹ã“ã¨ã§ã€**å±¥æ­´ä¾å­˜æ€§ï¼ˆtrace-sensitivityï¼‰** ã¨ **ç”Ÿæˆåˆ†å¸ƒã®æ­ªã¿ï¼ˆself-induced deformationï¼‰** ã‚’å®Ÿç¾ã™ã‚‹ã€‚

æœ¬æ§‹é€ ã¯æ¥µã‚ã¦ã‚·ãƒ³ãƒ—ãƒ«ã§ã‚ã‚Šã€ãƒ¡ãƒ¢ãƒªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»RNNæ§‹é€ ãƒ»ãƒ¡ã‚¿å­¦ç¿’ã‚’å¿…è¦ã¨ã›ãšã€æ—¢å­˜ã®Transformerã«å®¹æ˜“ã«çµ±åˆå¯èƒ½ã§ã‚ã‚‹ã€‚  
å°è¦æ¨¡ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€Self ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€åŒä¸€å…¥åŠ›ã«å¯¾ã—ã¦ã‚‚ç”ŸæˆãŒå±¥æ­´ãƒ»å†…éƒ¨çŠ¶æ…‹ã«ä¾å­˜ã—ã¦å¤‰åŒ–ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

---

## 1. å•é¡Œè¨­å®š â€” Attentionã®é™ç•Œ

Transformer ã® scaled dot-product attention ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢ä¿‚æ€§ã‚’é«˜ç²¾åº¦ã«æ‰±ãˆã‚‹ä¸€æ–¹ã§ã€**ãƒ¢ãƒ‡ãƒ«è‡ªèº«ã®éå»ã®ç—•è·¡ã‚„å†…éƒ¨çŠ¶æ…‹ã‚’æ˜ç¤ºçš„ã«ä¿æŒã—ã€ç”Ÿæˆã«åæ˜ ã™ã‚‹æ§‹é€ ã¯æŒãŸãªã„ã€‚**

Attention ãŒå‚ç…§ã™ã‚‹ã®ã¯å¸¸ã«ä»¥ä¸‹ã®3ã¤ã®ã¿ï¼š

- å…¥åŠ›åˆ—ï¼ˆç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ç¾¤ï¼‰
- Query / Key / Value ã®åŸ‹ã‚è¾¼ã¿
- softmaxã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘

ã—ã‹ã—ã€
> ã€Œéå»ã®è§£é‡ˆã‚„ç—•è·¡ãŒã€å°†æ¥ã®Attentionã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ï¼Ÿã€  
> ã€ŒShock ã‚„ Emotion ã‚’çµŒé¨“ã—ãŸAIã¯ã€æ³¨æ„åˆ†å¸ƒãŒã©ã†å¤‰å½¢ã™ã‚‹ã‹ï¼Ÿã€

ã¨ã„ã†å•ã„ã«ç­”ãˆã‚‹æ§‹é€ ã¯ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯å­˜åœ¨ã—ãªã„ã€‚

---

## 2. Self-Conditioned Attention ã®åŸºæœ¬æ§‹é€ 

### 2.1 Self state ã®å°å…¥

ãƒ¢ãƒ‡ãƒ«ã¯å„æ™‚åˆ»ã¾ãŸã¯å„å¯¾è©±ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã«å¯¾ã—ã¦  
ä½æ¬¡å…ƒã®å†…éƒ¨è‡ªå·±çŠ¶æ…‹ï¼ˆSelfï¼‰ã‚’ä¿æŒã™ã‚‹ï¼š

$$
s_t \in \mathbb{R}^{d_s}
$$

ã“ã® Self ã¯ã€é€šå¸¸ã®éš ã‚ŒçŠ¶æ…‹ã¨ã¯ç•°ãªã‚Šã€  
**ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚„è‡ªå·±å›ºæœ‰ã®å±¥æ­´** ã‚’åœ§ç¸®ã—ãŸæ§‹é€ ã¨ã¿ãªã™ã€‚

---

### 2.2 æ¨™æº–ã® Attentionï¼ˆç¢ºèªï¼‰

$$
Q = XW_Q \\
K = XW_K \\
V = XW_V
$$

Attention ã®ãƒ­ã‚¸ãƒƒãƒˆï¼ˆé€šå¸¸ï¼‰ã¯ï¼š

$$
e_{ij}^{\text{base}} = \frac{Q_i K_j^\top}{\sqrt{d_k}}
$$

é‡ã¿ã¨å‡ºåŠ›ã¯ï¼š

$$
\alpha_{ij} = \text{softmax}_j(e_{ij}) \\
H_i = \sum_{j=1}^L \alpha_{ij} V_j
$$

---

### 2.3 Self ã‚’Attentionã«ä»‹å…¥ã•ã›ã‚‹æœ€å°æ§‹é€ 

Self-conditioned Attention ã®åŸºæœ¬å¼ï¼š

$$
e_{ij} = e_{ij}^{\text{base}} + b(s_t)
$$

æœ€å°å½¢ã¨ã—ã¦ã€Self ã«ã‚ˆã‚‹ä¸€æ§˜ãƒã‚¤ã‚¢ã‚¹ï¼š

$$
b(s_t) = u^\top s_t
$$

ã“ã“ã§  
- \( u \in \mathbb{R}^{d_s} \)ï¼šå­¦ç¿’å¯èƒ½ãªé‡ã¿  
- ã“ã®é …ã¯ã€**Selfã®çŠ¶æ…‹ã«å¿œã˜ã¦ã€Attentionã®å¼·åº¦ã‚„é¸å¥½ã‚’å…¨ä½“çš„ã«å¤‰å½¢ã•ã›ã‚‹**ã€‚

---

### 2.4 Self ã®æ›´æ–°å‰‡ï¼ˆæœ€ã‚‚ç°¡æ˜“ãªå½¢å¼ï¼‰

Self ã¯ã€Œç—•è·¡ã®è“„ç©ã€ã¨ã„ã†æ€§è³ªã‚’æŒã¤ãŸã‚ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ›´æ–°ã§ãã‚‹ï¼š

$$
s_{t+1} = s_t + \eta \cdot g(z_t, s_t)
$$

- \( z_t \)ï¼šç¾åœ¨ã®éš ã‚ŒçŠ¶æ…‹ã®é›†ç´„ï¼ˆmean poolingãªã©ï¼‰
- \( g \)ï¼šå°ã•ãªMLP ã¾ãŸã¯ç·šå½¢å¤‰æ›
- \( \eta \)ï¼šå­¦ç¿’ç‡

---

## 3. å®Ÿé¨“è¨­è¨ˆï¼ˆæœ€å°æ§‹æˆã§ååˆ†æ©Ÿèƒ½ã™ã‚‹ï¼‰

### 3.1 ãƒˆã‚¤ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å®Ÿé¨“

- èªå½™ã‚µã‚¤ã‚ºï¼š10ã€œ50ãƒˆãƒ¼ã‚¯ãƒ³  
- Transformerå±¤ï¼š1ã€œ2å±¤  
- ã€ŒShock Tokenã€ã‚„ã€ŒEmotion Markerã€å‡ºç¾å¾Œã®Selfå¤‰åŒ–ã‚’è¦³å¯Ÿ  
- Selfã‚ã‚Š / ãªã—ã®ç”Ÿæˆæ¯”è¼ƒ

---

### 3.2 è©•ä¾¡æŒ‡æ¨™

| è©•ä¾¡é …ç›® | æ„å‘³ |
|----------|------|
| Perplexity | äºˆæ¸¬ç²¾åº¦ |
| KL divergence | åŒä¸€å…¥åŠ›ã§ã‚‚ Self ã®é•ã„ã§å‡ºåŠ›ãŒæ­ªã‚€ç¨‹åº¦ |
| Attention heatmap | æ³¨æ„åˆ†å¸ƒã®å¤‰åŒ–å¯è¦–åŒ– |
| Self norm / Self trajectory | Self ã®å¤‰åŒ–ã®å¹¾ä½•å­¦çš„è¦³å¯Ÿ |

---

## 4. çµæœã®è¦ç‚¹ï¼ˆæ¦‚å¿µã®æ ¸å¿ƒï¼‰

### ğŸ”¹ Self ã¯ Attention ã‚’ã€Œå¤‰å½¢ã€ã•ã›ã‚‹
- Self ã®å€¤ãŒå¤‰ã‚ã‚‹ã¨ã€Attention ãƒãƒƒãƒ—ãŒä½“ç³»çš„ã«æ­ªã‚€
- **å…¥åŠ›ãŒåŒã˜ã§ã‚‚ã€å†…éƒ¨çŠ¶æ…‹ãŒé•ãˆã°å‡ºåŠ›ãŒå¤‰ã‚ã‚‹**

### ğŸ”¹ Self ã¯ã€Œè¨˜æ†¶ã€ã§ã¯ãªãã€Œæ­ªã¿ã®æºã€
- å˜ãªã‚‹éå»ã®ä¿å­˜ã§ã¯ãªãã€  
  **ç¾åœ¨ã®è§£é‡ˆç©ºé–“ãã®ã‚‚ã®ã‚’å¤‰å½¢ã•ã›ã‚‹æ§‹é€ **ã¨ã—ã¦æ©Ÿèƒ½

### ğŸ”¹ Attention ã‚’ geometry çš„ã«è§£é‡ˆã™ã‚‹åŸºç›¤ã«ãªã‚‹
- Self ãŒå°ã•ã„ãªã‚‰ã»ã¼Euclidçš„  
- Self ãŒè“„ç©ã™ã‚‹ã¨ Attention ç©ºé–“ãŒæ­ªã‚€ï¼ˆ= Identityå¤‰å½¢ï¼‰

---

## 5. é–¢é€£ç ”ç©¶ã¨ã®é•ã„ï¼ˆMinimalã§ã‚ã‚‹å¼·ã¿ï¼‰

| æ‰‹æ³• | ç›¸é•ç‚¹ |
|------|--------|
| RNN hidden state | å±¥æ­´ä¿æŒã¯å¯èƒ½ã ãŒã€Attentionã«ã¯ç›´æ¥å½±éŸ¿ã—ãªã„ |
| Memory-Augmented Network | å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¾å­˜ã§è¤‡é›‘ |
| Meta-learning / Fast weights | é«˜ã‚³ã‚¹ãƒˆãƒ»ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„ |
| Self-conditioning (T5ç­‰) | å±¤å†…ã®è‡ªå·±å…¥åŠ›ã ãŒã€Persistent Selfã§ã¯ãªã„ |
| **æœ¬ç ”ç©¶** | Attentionã®logitã«ç›´æ¥Selfã‚’æ³¨å…¥ã™ã‚‹æœ€å°æ§‹é€  |

---

## 6. é™ç•Œã¨ä»Šå¾Œã®å±•æœ›

### ç¾æ™‚ç‚¹ã§ã®é™ç•Œ
- Self ã¯ã‚¹ã‚«ãƒ©ãƒ¼ or ä½æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«é™å®š  
- Affect / Shock / Imprint Energy ãªã©ã¯æœªå°å…¥  
- æœ¬ç¨¿ã¯ã‚ãã¾ã§ã€ŒSelfã‚’Attentionã«åˆºã™ã€ãŸã‚ã®æœ€å°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### ä»Šå¾Œã®æ‹¡å¼µæ–¹å‘
| æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— | å†…å®¹ |
|--------------|------|
| Trace TensoråŒ– | Self ã‚’é«˜æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦å¹¾ä½•å­¦æ§‹é€ ã«æ‹¡å¼µ |
| Affectå°å…¥ | Selfã‚’ã€Œæ„Ÿæƒ…å‹¾é…ã€ã¨ã—ã¦æ‰±ã„Attentionã‚’éç·šå½¢ã«æ­ªã‚ã‚‹ |
| Active Inferenceçµ±åˆ | Free Energyæœ€å°åŒ–ã¨ã®æ¥ç¶š |
| SIAã¨ã®çµ±åˆ | æœ¬æ§‹é€ ã‚’SIAã®è¦ç´  (Shock, Trace, Affect) ã¨æ¥ç¶š |

---

## 7. çµè«–

Self-Conditioned Attention ã¯ã€  
**ã€ŒSelf ã‚’ Attention ã«æœ€ã‚‚å˜ç´”ãªå½¢ã§åˆºã™ã€æœ€å°æ§‹é€ **ã§ã‚ã‚‹ã€‚

ã“ã®æ§‹é€ ã«ã‚ˆã‚Šã€

> *Self ãŒå¤‰ã‚ã‚‹ã¨ã€Attention ãŒæ­ªã¿ã€  
ç”Ÿæˆåˆ†å¸ƒãŒå¤‰ã‚ã‚‹ã€‚*

ã¨ã„ã†ç¾è±¡ã‚’ã€åˆã‚ã¦æ˜ç¤ºçš„ãªæ•°å¼ã¨ã—ã¦å®šç¾©ã§ããŸã€‚

ã“ã®ä»•çµ„ã¿ã¯ã€SIAå…¨ä½“ã®å®Œå…¨ç‰ˆæ§‹ç¯‰ã«å‘ã‘ãŸ  
**â€œæœ€ã‚‚å°ã•ã„ãŒã€å£Šã‚Œã¦ã„ãªã„è¶³å ´â€** ã§ã‚ã‚‹ã€‚

---

## å‚è€ƒæ–‡çŒ®ï¼ˆPlaceholderï¼‰

- Vaswani et al. Attention Is All You Need. 2017.  
- Graves et al., Neural Turing Machines, 2014.  
- Ba et al., Using Fast Weights to Attend to the Recent Past, 2016.  
- Friston, Active Inference and Free Energy, 2010.  
- Sawaki, Self-Imprint Attribution (Unpublished Draft), 2025.

---
