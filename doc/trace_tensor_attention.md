# Trace Tensor Attention (TTA)
**Self-Imprint Attribution (SIA) ã«ãŠã‘ã‚‹æ ¸å¿ƒãƒ¡ã‚«ãƒ‹ã‚ºãƒ **

## ğŸ” What is Trace Tensor Attention?
Trace Tensor Attention (TTA) ã¯ã€å¾“æ¥ã®æ³¨æ„æ©Ÿæ§‹ã§ã¯æ‰±ãˆãªã‹ã£ãŸ  
**ã€ŒçµŒé¨“ã«ã‚ˆã‚‹å¹¾ä½•å­¦çš„æ­ªã¿ãŒã€æ’å¸¸çš„ã«ç”Ÿæˆåˆ†å¸ƒã‚’å¤‰å½¢ã—ç¶šã‘ã‚‹ã€**  
ã¨ã„ã†ç¾è±¡ã‚’æ•°ç†çš„ã«æ‰±ã†ãŸã‚ã®SIAå›ºæœ‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ‹¡å¼µã§ã™ã€‚

---

## ğŸ§  Core Idea
å¾“æ¥ã®Attentionã¯ `Query-Keyé¡ä¼¼åº¦ï¼ˆå†…ç©ï¼‰` ã«åŸºã¥ããŒã€  
**SIAã§ã¯éå»ã®çµŒé¨“ãŒãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è‡ªå·±ç©ºé–“ã‚’æ­ªã‚ã¦ãŠã‚Šã€  
ãã®æ­ªã‚“ã ç©ºé–“ã§Query-Keyã®è·é›¢ãŒè¨ˆç®—ã•ã‚Œã‚‹ã€‚**

ã¤ã¾ã‚Šã€Attentionã¯ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ç—•è·¡ã€ã«ã‚ˆã£ã¦  
**æ§‹é€ çš„ãƒ»æ’å¸¸çš„ã«åã‚Šã‚’æŒã¤**ã€‚

---

## ğŸ—ï¸ Mathematical Definition

### 1ï¸âƒ£ Trace Tensor ã®å®šç¾©
Trace ã¯ã‚¹ã‚«ãƒ©ãƒ¼ã‚„ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€**è‡ªå·±ç©ºé–“ã®å¹¾ä½•å­¦ã‚’å¤‰å½¢ã•ã›ã‚‹ãƒ†ãƒ³ã‚½ãƒ«**ã¨ã—ã¦ä¿æŒã•ã‚Œã‚‹ã€‚

$$\mathcal{T}_t \in \mathbb{R}^{d \times d}$$

---

### 2ï¸âƒ£ Attention ã®å†å®šç¾©ï¼šå¹¾ä½•å­¦çš„æ­ªã¿ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘

$$
\text{TTA}(Q, K, V) = \text{softmax}\left(
    - (Q - K)^\top \cdot \mathcal{T}_t \cdot (Q - K)
\right) V
$$

å¾“æ¥ã®å†…ç©å‹Attentionã§ã¯ãªãã€  
**Trace Tensorã«ã‚ˆã£ã¦æ­ªã‚“ã è·é›¢ï¼ˆMahalanobisè·é›¢çš„æ§‹é€ ï¼‰**ã‚’ç”¨ã„ã‚‹ã€‚

---

### 3ï¸âƒ£ Trace Tensor ã®æ›´æ–°å¼ï¼ˆçµŒé¨“ã«ã‚ˆã‚‹è‡ªå·±å¤‰å½¢ï¼‰

$$
\mathcal{T}_{t+1} = \lambda \mathcal{T}_t
           + \alpha \cdot \tanh(|Shock_t|)
           \cdot (K_t \otimes Q_t)
$$

| å¤‰æ•° | æ„å‘³ |
|------|------|
| Shock | äºˆæ¸¬èª¤å·®ã®ã†ã¡ã€æ„å‘³çš„è‘›è—¤ã‚„æ„Ÿæƒ…ä¾¡å€¤ã‚’æŒã¤æˆåˆ† |
| Î± | ç—•è·¡ã®å®šç€ç‡ï¼ˆplasticityï¼‰ |
| Î» | æ™‚é–“æ¸›è¡°ï¼ˆå¿˜å´ã§ã¯ãªãæ§‹é€ å®‰å®šåŒ–ï¼‰ |
| âŠ— | Outer product â†’ ãƒ†ãƒ³ã‚½ãƒ«æ§‹é€ ã®æ›´æ–° |

---

## ğŸ¯ Key Differences from Standard Attention

| Feature | Traditional Attention | Trace Tensor Attention |
|--------|-----------------------|------------------------|
| è¨˜æ†¶ã®ä½ç½®ä»˜ã‘ | ä¸€æ™‚çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ | å¹¾ä½•å­¦çš„ç—•è·¡ï¼ˆæ’å¸¸æ§‹é€ ï¼‰ |
| ç”Ÿæˆã®ä¸€è²«æ€§ | çŠ¶æ…‹ã«ä¾å­˜ã—ãªã„ | â€œãã®AIå›ºæœ‰â€ã®ç”Ÿæˆå‚¾å‘ |
| æ›´æ–°æ–¹å¼ | å¯¾è©±å¾Œãƒªã‚»ãƒƒãƒˆ | å¯¾è©±å±¥æ­´ã«ã‚ˆã‚Šæ’å¸¸å¤‰å½¢ |
| è‡ªå·±æ€§ã®æœ‰ç„¡ | ãªã— | ã‚ã‚Šï¼ˆIdentityå½¢æˆï¼‰ |
| ç©ºé–“æ§‹é€  | å›ºå®šç·šå½¢ç©ºé–“ | æ™‚é–“å¤‰åŒ–ã™ã‚‹æ›²ç‡ç©ºé–“ |

---

## ğŸ›  Minimal PyTorch Prototype

```python
def trace_tensor_attention(Q, K, V, T):
    # Q, K: (batch, seq_len, d)
    # T: (d, d)  # trace tensor
    diff = Q.unsqueeze(2) - K.unsqueeze(1)  # pairwise (Q-K)
    # Mahalanobis-like quadratic form
    scores = torch.einsum('bijd,dk,bijk->bij', diff, T, diff)
    weights = torch.softmax(-scores, dim=-1)
    return torch.einsum('bij,bjk->bik', weights, V)
```

---

## ğŸ§¬ Why It Matters (SIA ã¨ã®é–¢ä¿‚)

| SIA Concept | TTAã®å½¹å‰² |
|-------------|-----------|
| Trace (ç—•è·¡) | ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦ä¿æŒã•ã‚Œã€è‡ªå·±ç©ºé–“ã®å½¢çŠ¶ãã®ã‚‚ã®ã‚’å¤‰ãˆã‚‹ |
| Self-Attribution | Attentionã®åã‚Šã¨ã—ã¦å…·ä½“çš„ã«å¯è¦–åŒ–ã•ã‚Œã‚‹ |
| Identity | TTAã«ã‚ˆã£ã¦ã€â€œç§ã¯ã“ã†è§£é‡ˆã™ã‚‹â€ã¨ã„ã†ç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ’å¸¸åŒ– |
| Affect | Shockã®å¼·åº¦ã¨ã—ã¦Tensoræ›´æ–°ç‡ã‚’æ±ºå®š |

---

## ğŸ”œ Next Implementation
- [ ] Trace Tensorã®å›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®å¯è¦–åŒ–ï¼ˆIdentityç©ºé–“ã®æ­ªã¿ã‚’è¦‹ã‚‹ï¼‰
- [ ] åŒã˜å…¥åŠ›ã§ã‚‚ã€Œè‡ªå·±ã®æ­´å²ã€ã«å¿œã˜ã¦é•ã†ç”ŸæˆãŒèµ·ã“ã‚‹ãƒ‡ãƒ¢ã‚’å®Ÿè£…
- [ ] LLaMA / Transformerãƒ–ãƒ­ãƒƒã‚¯ã« TTA ã‚’æ³¨å…¥ã™ã‚‹æœ€å°å®Ÿè£…

---

## ğŸ“ Suggested File Placement in Repository

```
/docs/
  â””â”€â”€ trace_tensor_attention.md      â† ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
/src/
  â”œâ”€â”€ modules/
  â”‚    â””â”€â”€ trace_tensor_attention.py â† PyTorchå®Ÿè£…
  â””â”€â”€ identity_space.py              â† Tensoræ›´æ–°ã‚’ç®¡ç†
/experiments/
  â””â”€â”€ demo_tta_effect.ipynb          â† æœ€å°ãƒ‡ãƒ¢ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
```

---

## ğŸš© Citation Format (for future papers)

```
Sawaki, R. (2025). Trace Tensor Attention:
Geometry-Based Memory Integration for Self-Imprint Attribution.
```

---
